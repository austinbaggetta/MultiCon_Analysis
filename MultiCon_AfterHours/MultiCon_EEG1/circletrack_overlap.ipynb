{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "from tqdm import tqdm\n",
    "from natsort import natsort_keygen\n",
    "import os\n",
    "import datetime\n",
    "import sys\n",
    "from scipy.stats import zscore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('/media/caishuman/csstorage3/Austin/CircleTrack/CircleTrackAnalysis')\n",
    "import cell_overlap as co\n",
    "import plotting_functions as pf\n",
    "import circletrack_neural as ctn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set parameters\n",
    "## Mouse list\n",
    "mouse_list = ['mc_EEG1_01', 'mc_EEG1_02']\n",
    "## Load mappings of a specific param_distance from cross_registration results \n",
    "param_distance = 5\n",
    "## Specify session type; one of 'pre', 'circletrack', 'post'\n",
    "session_type = 'circletrack'\n",
    "## Set key file\n",
    "key_file = '{}_data_keys.yml'.format(session_type)\n",
    "## Set path\n",
    "path = '/media/caishuman/csstorage3/Austin/CircleTrack/MultiCon_AfterHours/MultiCon_EEG1/circletrack_data'\n",
    "## Create session list\n",
    "session_list_one = ['A1', 'A2', 'A3', 'A4', 'A5', 'B1', 'B2', 'B3', 'B4', 'B5', 'C1', 'C2', 'C3', 'C4', 'C5', 'D1', 'D2', 'D3', 'D4', 'D5']\n",
    "session_list_two = ['A1', 'A2', 'A3', 'A4', 'A5', 'B1', 'B2', 'B3', 'B4', 'B5', 'C1', 'C2', 'C3', 'C4', 'C5', 'A2_1', 'A2_2', 'A2_3', 'A2_4', 'A2_5']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/caishuman/csstorage3/Austin/CircleTrack/CircleTrackAnalysis/circletrack_neural.py:34: RuntimeWarning: Failed to open Zarr store with consolidated metadata, falling back to try reading non-consolidated metadata. This is typically much slower for opening a dataset. To silence this warning, consider:\n",
      "1. Consolidating metadata in this existing store with zarr.consolidate_metadata().\n",
      "2. Explicitly setting consolidated=False, to avoid trying to read consolidate metadata, or\n",
      "3. Explicitly setting consolidated=True, to raise an error in this case instead of falling back to try reading non-consolidated metadata.\n",
      "  xr.open_zarr(pjoin(dpath, d))\n",
      "/media/caishuman/csstorage3/Austin/CircleTrack/CircleTrackAnalysis/circletrack_neural.py:34: RuntimeWarning: Failed to open Zarr store with consolidated metadata, falling back to try reading non-consolidated metadata. This is typically much slower for opening a dataset. To silence this warning, consider:\n",
      "1. Consolidating metadata in this existing store with zarr.consolidate_metadata().\n",
      "2. Explicitly setting consolidated=False, to avoid trying to read consolidate metadata, or\n",
      "3. Explicitly setting consolidated=True, to raise an error in this case instead of falling back to try reading non-consolidated metadata.\n",
      "  xr.open_zarr(pjoin(dpath, d))\n",
      "/media/caishuman/csstorage3/Austin/CircleTrack/CircleTrackAnalysis/circletrack_neural.py:34: RuntimeWarning: Failed to open Zarr store with consolidated metadata, falling back to try reading non-consolidated metadata. This is typically much slower for opening a dataset. To silence this warning, consider:\n",
      "1. Consolidating metadata in this existing store with zarr.consolidate_metadata().\n",
      "2. Explicitly setting consolidated=False, to avoid trying to read consolidate metadata, or\n",
      "3. Explicitly setting consolidated=True, to raise an error in this case instead of falling back to try reading non-consolidated metadata.\n",
      "  xr.open_zarr(pjoin(dpath, d))\n",
      "/media/caishuman/csstorage3/Austin/CircleTrack/CircleTrackAnalysis/circletrack_neural.py:34: RuntimeWarning: Failed to open Zarr store with consolidated metadata, falling back to try reading non-consolidated metadata. This is typically much slower for opening a dataset. To silence this warning, consider:\n",
      "1. Consolidating metadata in this existing store with zarr.consolidate_metadata().\n",
      "2. Explicitly setting consolidated=False, to avoid trying to read consolidate metadata, or\n",
      "3. Explicitly setting consolidated=True, to raise an error in this case instead of falling back to try reading non-consolidated metadata.\n",
      "  xr.open_zarr(pjoin(dpath, d))\n",
      "/media/caishuman/csstorage3/Austin/CircleTrack/CircleTrackAnalysis/circletrack_neural.py:34: RuntimeWarning: Failed to open Zarr store with consolidated metadata, falling back to try reading non-consolidated metadata. This is typically much slower for opening a dataset. To silence this warning, consider:\n",
      "1. Consolidating metadata in this existing store with zarr.consolidate_metadata().\n",
      "2. Explicitly setting consolidated=False, to avoid trying to read consolidate metadata, or\n",
      "3. Explicitly setting consolidated=True, to raise an error in this case instead of falling back to try reading non-consolidated metadata.\n",
      "  xr.open_zarr(pjoin(dpath, d))\n",
      "/media/caishuman/csstorage3/Austin/CircleTrack/CircleTrackAnalysis/circletrack_neural.py:34: RuntimeWarning: Failed to open Zarr store with consolidated metadata, falling back to try reading non-consolidated metadata. This is typically much slower for opening a dataset. To silence this warning, consider:\n",
      "1. Consolidating metadata in this existing store with zarr.consolidate_metadata().\n",
      "2. Explicitly setting consolidated=False, to avoid trying to read consolidate metadata, or\n",
      "3. Explicitly setting consolidated=True, to raise an error in this case instead of falling back to try reading non-consolidated metadata.\n",
      "  xr.open_zarr(pjoin(dpath, d))\n",
      "/media/caishuman/csstorage3/Austin/CircleTrack/CircleTrackAnalysis/circletrack_neural.py:34: RuntimeWarning: Failed to open Zarr store with consolidated metadata, falling back to try reading non-consolidated metadata. This is typically much slower for opening a dataset. To silence this warning, consider:\n",
      "1. Consolidating metadata in this existing store with zarr.consolidate_metadata().\n",
      "2. Explicitly setting consolidated=False, to avoid trying to read consolidate metadata, or\n",
      "3. Explicitly setting consolidated=True, to raise an error in this case instead of falling back to try reading non-consolidated metadata.\n",
      "  xr.open_zarr(pjoin(dpath, d))\n",
      "/media/caishuman/csstorage3/Austin/CircleTrack/CircleTrackAnalysis/circletrack_neural.py:34: RuntimeWarning: Failed to open Zarr store with consolidated metadata, falling back to try reading non-consolidated metadata. This is typically much slower for opening a dataset. To silence this warning, consider:\n",
      "1. Consolidating metadata in this existing store with zarr.consolidate_metadata().\n",
      "2. Explicitly setting consolidated=False, to avoid trying to read consolidate metadata, or\n",
      "3. Explicitly setting consolidated=True, to raise an error in this case instead of falling back to try reading non-consolidated metadata.\n",
      "  xr.open_zarr(pjoin(dpath, d))\n",
      "/media/caishuman/csstorage3/Austin/CircleTrack/CircleTrackAnalysis/circletrack_neural.py:34: RuntimeWarning: Failed to open Zarr store with consolidated metadata, falling back to try reading non-consolidated metadata. This is typically much slower for opening a dataset. To silence this warning, consider:\n",
      "1. Consolidating metadata in this existing store with zarr.consolidate_metadata().\n",
      "2. Explicitly setting consolidated=False, to avoid trying to read consolidate metadata, or\n",
      "3. Explicitly setting consolidated=True, to raise an error in this case instead of falling back to try reading non-consolidated metadata.\n",
      "  xr.open_zarr(pjoin(dpath, d))\n",
      "/media/caishuman/csstorage3/Austin/CircleTrack/CircleTrackAnalysis/circletrack_neural.py:34: RuntimeWarning: Failed to open Zarr store with consolidated metadata, falling back to try reading non-consolidated metadata. This is typically much slower for opening a dataset. To silence this warning, consider:\n",
      "1. Consolidating metadata in this existing store with zarr.consolidate_metadata().\n",
      "2. Explicitly setting consolidated=False, to avoid trying to read consolidate metadata, or\n",
      "3. Explicitly setting consolidated=True, to raise an error in this case instead of falling back to try reading non-consolidated metadata.\n",
      "  xr.open_zarr(pjoin(dpath, d))\n",
      "/media/caishuman/csstorage3/Austin/CircleTrack/CircleTrackAnalysis/circletrack_neural.py:34: RuntimeWarning: Failed to open Zarr store with consolidated metadata, falling back to try reading non-consolidated metadata. This is typically much slower for opening a dataset. To silence this warning, consider:\n",
      "1. Consolidating metadata in this existing store with zarr.consolidate_metadata().\n",
      "2. Explicitly setting consolidated=False, to avoid trying to read consolidate metadata, or\n",
      "3. Explicitly setting consolidated=True, to raise an error in this case instead of falling back to try reading non-consolidated metadata.\n",
      "  xr.open_zarr(pjoin(dpath, d))\n",
      "/media/caishuman/csstorage3/Austin/CircleTrack/CircleTrackAnalysis/circletrack_neural.py:34: RuntimeWarning: Failed to open Zarr store with consolidated metadata, falling back to try reading non-consolidated metadata. This is typically much slower for opening a dataset. To silence this warning, consider:\n",
      "1. Consolidating metadata in this existing store with zarr.consolidate_metadata().\n",
      "2. Explicitly setting consolidated=False, to avoid trying to read consolidate metadata, or\n",
      "3. Explicitly setting consolidated=True, to raise an error in this case instead of falling back to try reading non-consolidated metadata.\n",
      "  xr.open_zarr(pjoin(dpath, d))\n",
      "/media/caishuman/csstorage3/Austin/CircleTrack/CircleTrackAnalysis/circletrack_neural.py:34: RuntimeWarning: Failed to open Zarr store with consolidated metadata, falling back to try reading non-consolidated metadata. This is typically much slower for opening a dataset. To silence this warning, consider:\n",
      "1. Consolidating metadata in this existing store with zarr.consolidate_metadata().\n",
      "2. Explicitly setting consolidated=False, to avoid trying to read consolidate metadata, or\n",
      "3. Explicitly setting consolidated=True, to raise an error in this case instead of falling back to try reading non-consolidated metadata.\n",
      "  xr.open_zarr(pjoin(dpath, d))\n",
      "/media/caishuman/csstorage3/Austin/CircleTrack/CircleTrackAnalysis/circletrack_neural.py:34: RuntimeWarning: Failed to open Zarr store with consolidated metadata, falling back to try reading non-consolidated metadata. This is typically much slower for opening a dataset. To silence this warning, consider:\n",
      "1. Consolidating metadata in this existing store with zarr.consolidate_metadata().\n",
      "2. Explicitly setting consolidated=False, to avoid trying to read consolidate metadata, or\n",
      "3. Explicitly setting consolidated=True, to raise an error in this case instead of falling back to try reading non-consolidated metadata.\n",
      "  xr.open_zarr(pjoin(dpath, d))\n",
      "/media/caishuman/csstorage3/Austin/CircleTrack/CircleTrackAnalysis/circletrack_neural.py:34: RuntimeWarning: Failed to open Zarr store with consolidated metadata, falling back to try reading non-consolidated metadata. This is typically much slower for opening a dataset. To silence this warning, consider:\n",
      "1. Consolidating metadata in this existing store with zarr.consolidate_metadata().\n",
      "2. Explicitly setting consolidated=False, to avoid trying to read consolidate metadata, or\n",
      "3. Explicitly setting consolidated=True, to raise an error in this case instead of falling back to try reading non-consolidated metadata.\n",
      "  xr.open_zarr(pjoin(dpath, d))\n",
      "/media/caishuman/csstorage3/Austin/CircleTrack/CircleTrackAnalysis/circletrack_neural.py:34: RuntimeWarning: Failed to open Zarr store with consolidated metadata, falling back to try reading non-consolidated metadata. This is typically much slower for opening a dataset. To silence this warning, consider:\n",
      "1. Consolidating metadata in this existing store with zarr.consolidate_metadata().\n",
      "2. Explicitly setting consolidated=False, to avoid trying to read consolidate metadata, or\n",
      "3. Explicitly setting consolidated=True, to raise an error in this case instead of falling back to try reading non-consolidated metadata.\n",
      "  xr.open_zarr(pjoin(dpath, d))\n",
      "/media/caishuman/csstorage3/Austin/CircleTrack/CircleTrackAnalysis/circletrack_neural.py:34: RuntimeWarning: Failed to open Zarr store with consolidated metadata, falling back to try reading non-consolidated metadata. This is typically much slower for opening a dataset. To silence this warning, consider:\n",
      "1. Consolidating metadata in this existing store with zarr.consolidate_metadata().\n",
      "2. Explicitly setting consolidated=False, to avoid trying to read consolidate metadata, or\n",
      "3. Explicitly setting consolidated=True, to raise an error in this case instead of falling back to try reading non-consolidated metadata.\n",
      "  xr.open_zarr(pjoin(dpath, d))\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"not all values found in index 'frame'\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m/media/caishuman/csstorage3/Austin/CircleTrack/MultiCon_AfterHours/MultiCon_EEG1/circletrack_data/python_analysis/circletrack_overlap.ipynb Cell 4\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bcarbon.hcsm.mssm.edu/media/caishuman/csstorage3/Austin/CircleTrack/MultiCon_AfterHours/MultiCon_EEG1/circletrack_data/python_analysis/circletrack_overlap.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m mouse_neural \u001b[39m=\u001b[39m {}\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bcarbon.hcsm.mssm.edu/media/caishuman/csstorage3/Austin/CircleTrack/MultiCon_AfterHours/MultiCon_EEG1/circletrack_data/python_analysis/circletrack_overlap.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mfor\u001b[39;00m mouse \u001b[39min\u001b[39;00m mouse_list:\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bcarbon.hcsm.mssm.edu/media/caishuman/csstorage3/Austin/CircleTrack/MultiCon_AfterHours/MultiCon_EEG1/circletrack_data/python_analysis/circletrack_overlap.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m     sessions \u001b[39m=\u001b[39m ctn\u001b[39m.\u001b[39;49mimport_mouse_neural_data(path, mouse, key_file \u001b[39m=\u001b[39;49m key_file, session \u001b[39m=\u001b[39;49m \u001b[39m'\u001b[39;49m\u001b[39m20min\u001b[39;49m\u001b[39m'\u001b[39;49m, neural_type \u001b[39m=\u001b[39;49m \u001b[39m'\u001b[39;49m\u001b[39mspikes\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bcarbon.hcsm.mssm.edu/media/caishuman/csstorage3/Austin/CircleTrack/MultiCon_AfterHours/MultiCon_EEG1/circletrack_data/python_analysis/circletrack_overlap.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m     cell_dict \u001b[39m=\u001b[39m {}\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bcarbon.hcsm.mssm.edu/media/caishuman/csstorage3/Austin/CircleTrack/MultiCon_AfterHours/MultiCon_EEG1/circletrack_data/python_analysis/circletrack_overlap.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m     \u001b[39mfor\u001b[39;00m session \u001b[39min\u001b[39;00m sessions:\n",
      "File \u001b[0;32m/media/caishuman/csstorage3/Austin/CircleTrack/CircleTrackAnalysis/circletrack_neural.py:209\u001b[0m, in \u001b[0;36mimport_mouse_neural_data\u001b[0;34m(path, mouse, key_file, session, neural_type, plot_frame_usage)\u001b[0m\n\u001b[1;32m    207\u001b[0m dpath \u001b[39m=\u001b[39m pjoin(dpath, \u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(mouse))\n\u001b[1;32m    208\u001b[0m \u001b[39mfor\u001b[39;00m date \u001b[39min\u001b[39;00m os\u001b[39m.\u001b[39mlistdir(dpath):\n\u001b[0;32m--> 209\u001b[0m     sessions[\u001b[39mlist\u001b[39m(keys\u001b[39m.\u001b[39mkeys())[\u001b[39mlist\u001b[39m(keys\u001b[39m.\u001b[39mvalues())\u001b[39m.\u001b[39mindex([date])]] \u001b[39m=\u001b[39m load_and_align_minian(path, mouse, date, session \u001b[39m=\u001b[39;49m session, neural_type \u001b[39m=\u001b[39;49m neural_type, plot_frame_usage \u001b[39m=\u001b[39;49m plot_frame_usage)\n\u001b[1;32m    210\u001b[0m \u001b[39mreturn\u001b[39;00m sessions\n",
      "File \u001b[0;32m/media/caishuman/csstorage3/Austin/CircleTrack/CircleTrackAnalysis/circletrack_neural.py:174\u001b[0m, in \u001b[0;36mload_and_align_minian\u001b[0;34m(path, mouse, date, session, neural_type, sigma, sampling_rate, downsample_further, downsample_factor, plot_frame_usage, frames_per_file)\u001b[0m\n\u001b[1;32m    172\u001b[0m     lined_up_timeframes \u001b[39m=\u001b[39m lined_up_timeframes[::downsample_factor]\n\u001b[1;32m    173\u001b[0m \u001b[39m## Select frames based on line-up timeframes\u001b[39;00m\n\u001b[0;32m--> 174\u001b[0m neural_activity \u001b[39m=\u001b[39m neural_activity\u001b[39m.\u001b[39;49msel(frame\u001b[39m=\u001b[39;49mlined_up_timeframes)\n\u001b[1;32m    175\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m    176\u001b[0m     neural_type \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39msmoothed\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    177\u001b[0m ):  \u001b[39m# this filtering must be done after the previous line because this converts neural_activity to numpy array\u001b[39;00m\n\u001b[1;32m    178\u001b[0m     neural_activity \u001b[39m=\u001b[39m gaussian_filter(neural_activity, sigma\u001b[39m=\u001b[39m(\u001b[39m1\u001b[39m, sigma))\n",
      "File \u001b[0;32m~/.conda/envs/calcium/lib/python3.10/site-packages/xarray/core/dataarray.py:1332\u001b[0m, in \u001b[0;36mDataArray.sel\u001b[0;34m(self, indexers, method, tolerance, drop, **indexers_kwargs)\u001b[0m\n\u001b[1;32m   1223\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msel\u001b[39m(\n\u001b[1;32m   1224\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m   1225\u001b[0m     indexers: Mapping[Any, Any] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1229\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mindexers_kwargs: Any,\n\u001b[1;32m   1230\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mDataArray\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m   1231\u001b[0m     \u001b[39m\"\"\"Return a new DataArray whose data is given by selecting index\u001b[39;00m\n\u001b[1;32m   1232\u001b[0m \u001b[39m    labels along the specified dimension(s).\u001b[39;00m\n\u001b[1;32m   1233\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1330\u001b[0m \u001b[39m    Dimensions without coordinates: points\u001b[39;00m\n\u001b[1;32m   1331\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1332\u001b[0m     ds \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_to_temp_dataset()\u001b[39m.\u001b[39;49msel(\n\u001b[1;32m   1333\u001b[0m         indexers\u001b[39m=\u001b[39;49mindexers,\n\u001b[1;32m   1334\u001b[0m         drop\u001b[39m=\u001b[39;49mdrop,\n\u001b[1;32m   1335\u001b[0m         method\u001b[39m=\u001b[39;49mmethod,\n\u001b[1;32m   1336\u001b[0m         tolerance\u001b[39m=\u001b[39;49mtolerance,\n\u001b[1;32m   1337\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mindexers_kwargs,\n\u001b[1;32m   1338\u001b[0m     )\n\u001b[1;32m   1339\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_from_temp_dataset(ds)\n",
      "File \u001b[0;32m~/.conda/envs/calcium/lib/python3.10/site-packages/xarray/core/dataset.py:2504\u001b[0m, in \u001b[0;36mDataset.sel\u001b[0;34m(self, indexers, method, tolerance, drop, **indexers_kwargs)\u001b[0m\n\u001b[1;32m   2443\u001b[0m \u001b[39m\"\"\"Returns a new dataset with each array indexed by tick labels\u001b[39;00m\n\u001b[1;32m   2444\u001b[0m \u001b[39malong the specified dimension(s).\u001b[39;00m\n\u001b[1;32m   2445\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2501\u001b[0m \u001b[39mDataArray.sel\u001b[39;00m\n\u001b[1;32m   2502\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   2503\u001b[0m indexers \u001b[39m=\u001b[39m either_dict_or_kwargs(indexers, indexers_kwargs, \u001b[39m\"\u001b[39m\u001b[39msel\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m-> 2504\u001b[0m pos_indexers, new_indexes \u001b[39m=\u001b[39m remap_label_indexers(\n\u001b[1;32m   2505\u001b[0m     \u001b[39mself\u001b[39;49m, indexers\u001b[39m=\u001b[39;49mindexers, method\u001b[39m=\u001b[39;49mmethod, tolerance\u001b[39m=\u001b[39;49mtolerance\n\u001b[1;32m   2506\u001b[0m )\n\u001b[1;32m   2507\u001b[0m \u001b[39m# TODO: benbovy - flexible indexes: also use variables returned by Index.query\u001b[39;00m\n\u001b[1;32m   2508\u001b[0m \u001b[39m# (temporary dirty fix).\u001b[39;00m\n\u001b[1;32m   2509\u001b[0m new_indexes \u001b[39m=\u001b[39m {k: v[\u001b[39m0\u001b[39m] \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m new_indexes\u001b[39m.\u001b[39mitems()}\n",
      "File \u001b[0;32m~/.conda/envs/calcium/lib/python3.10/site-packages/xarray/core/coordinates.py:421\u001b[0m, in \u001b[0;36mremap_label_indexers\u001b[0;34m(obj, indexers, method, tolerance, **indexers_kwargs)\u001b[0m\n\u001b[1;32m    414\u001b[0m indexers \u001b[39m=\u001b[39m either_dict_or_kwargs(indexers, indexers_kwargs, \u001b[39m\"\u001b[39m\u001b[39mremap_label_indexers\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    416\u001b[0m v_indexers \u001b[39m=\u001b[39m {\n\u001b[1;32m    417\u001b[0m     k: v\u001b[39m.\u001b[39mvariable\u001b[39m.\u001b[39mdata \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(v, DataArray) \u001b[39melse\u001b[39;00m v\n\u001b[1;32m    418\u001b[0m     \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m indexers\u001b[39m.\u001b[39mitems()\n\u001b[1;32m    419\u001b[0m }\n\u001b[0;32m--> 421\u001b[0m pos_indexers, new_indexes \u001b[39m=\u001b[39m indexing\u001b[39m.\u001b[39;49mremap_label_indexers(\n\u001b[1;32m    422\u001b[0m     obj, v_indexers, method\u001b[39m=\u001b[39;49mmethod, tolerance\u001b[39m=\u001b[39;49mtolerance\n\u001b[1;32m    423\u001b[0m )\n\u001b[1;32m    424\u001b[0m \u001b[39m# attach indexer's coordinate to pos_indexers\u001b[39;00m\n\u001b[1;32m    425\u001b[0m \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m indexers\u001b[39m.\u001b[39mitems():\n",
      "File \u001b[0;32m~/.conda/envs/calcium/lib/python3.10/site-packages/xarray/core/indexing.py:120\u001b[0m, in \u001b[0;36mremap_label_indexers\u001b[0;34m(data_obj, indexers, method, tolerance)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[39mfor\u001b[39;00m dim, index \u001b[39min\u001b[39;00m indexes\u001b[39m.\u001b[39mitems():\n\u001b[1;32m    119\u001b[0m     labels \u001b[39m=\u001b[39m grouped_indexers[dim]\n\u001b[0;32m--> 120\u001b[0m     idxr, new_idx \u001b[39m=\u001b[39m index\u001b[39m.\u001b[39;49mquery(labels, method\u001b[39m=\u001b[39;49mmethod, tolerance\u001b[39m=\u001b[39;49mtolerance)\n\u001b[1;32m    121\u001b[0m     pos_indexers[dim] \u001b[39m=\u001b[39m idxr\n\u001b[1;32m    122\u001b[0m     \u001b[39mif\u001b[39;00m new_idx \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/.conda/envs/calcium/lib/python3.10/site-packages/xarray/core/indexes.py:242\u001b[0m, in \u001b[0;36mPandasIndex.query\u001b[0;34m(self, labels, method, tolerance)\u001b[0m\n\u001b[1;32m    240\u001b[0m         indexer \u001b[39m=\u001b[39m get_indexer_nd(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindex, label, method, tolerance)\n\u001b[1;32m    241\u001b[0m         \u001b[39mif\u001b[39;00m np\u001b[39m.\u001b[39many(indexer \u001b[39m<\u001b[39m \u001b[39m0\u001b[39m):\n\u001b[0;32m--> 242\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mnot all values found in index \u001b[39m\u001b[39m{\u001b[39;00mcoord_name\u001b[39m!r}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    244\u001b[0m \u001b[39mreturn\u001b[39;00m indexer, \u001b[39mNone\u001b[39;00m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"not all values found in index 'frame'\""
     ]
    }
   ],
   "source": [
    "## Plot cells across sessions\n",
    "## Loop through mouse list\n",
    "mouse_neural = {}\n",
    "for mouse in mouse_list:\n",
    "    sessions = ctn.import_mouse_neural_data(path, mouse, key_file = key_file, session = '20min', neural_type = 'spikes')\n",
    "    cell_dict = {}\n",
    "    for session in sessions:\n",
    "        cell_dict[session] = len(sessions[session].unit_id)\n",
    "    mouse_neural[mouse] = cell_dict\n",
    "fig = pf.custom_graph_template(title = 'Cells per Session', x_title = 'Session', y_title = 'Number of Cells')\n",
    "for mouse in mouse_neural:\n",
    "    plot_data = pd.DataFrame(mouse_neural[mouse], index = [0])\n",
    "    if mouse == 'mc03':\n",
    "        plot_data.insert(0, 'Reversal2', np.nan)\n",
    "        plot_data.insert(1, 'Reversal3', np.nan)\n",
    "    plot_data = plot_data[session_list]\n",
    "    fig.add_trace(go.Scatter(x = session_list, y = plot_data.iloc[0, :], mode = 'lines', line_color = 'grey', line_width = 0.5, opacity = 0.5, name = mouse, showlegend = False))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plot pairwise pre_session heatmaps\n",
    "## Create subplots\n",
    "fig = make_subplots(rows = 1, cols = 2, subplot_titles = (mouse_list[0], mouse_list[1]))\n",
    "## Loop through each mouse and add plots to fig\n",
    "for mouse in tqdm(mouse_list):\n",
    "    mappings = pd.read_pickle('../../cross_registration_results/{}_data/{}/mappings_{}.pkl'.format(session_type, mouse, param_distance))\n",
    "    overlap = co.calculate_overlap(mappings)\n",
    "    if mouse == mouse_list[0]:\n",
    "        overlap = co.dates_to_days(overlap, '2022_10_19', days = 20)\n",
    "        #between_context_one = co.between_context_overlap(mouse = mouse_list[0], overlap = overlap, session_ids = ['A5', 'B5', 'C5', 'D5', 'R_A5'], session_type = 'pre')\n",
    "        ## Create heatmap matrix\n",
    "        matrix = overlap.pivot_table(index = 'session_id1', columns = 'session_id2', values = 'overlap')\n",
    "        matrix = matrix.sort_values(by = 'session_id1')\n",
    "        matrix = matrix.sort_values(by = 'session_id2', axis = 1)\n",
    "        fig.add_trace(go.Heatmap(z = matrix.values, x = matrix.index, y = matrix.columns, coloraxis = 'coloraxis'), row = 1, col = 1)\n",
    "    elif mouse == mouse_list[1]:\n",
    "        overlap = co.dates_to_days(overlap, '2022_10_21', days = 20)\n",
    "        #between_context_two = co.between_context_overlap(mouse = mouse_list[1], overlap = overlap, session_ids = ['A5', 'B5', 'C5', 'D5', 'R_A5'], session_type = 'pre')\n",
    "        ## Create heatmap matrix\n",
    "        matrix = overlap.pivot_table(index = 'session_id1', columns = 'session_id2', values = 'overlap')\n",
    "        matrix = matrix.sort_values(by = 'session_id1')\n",
    "        matrix = matrix.sort_values(by = 'session_id2', axis = 1)\n",
    "        fig.add_trace(go.Heatmap(z = matrix.values, x = matrix.index, y = matrix.columns, coloraxis = 'coloraxis'), row = 1, col = 2)\n",
    "## Figure alterations\n",
    "fig.update_layout(template = 'simple_white', width = 1600, height = 800, coloraxis = {'colorscale': 'Viridis'})\n",
    "fig.update_xaxes(title_text = 'Day', row = 1, col = 1)\n",
    "fig.update_xaxes(title_text = 'Day', row = 1, col = 2)\n",
    "fig.update_yaxes(title_text = 'Day', row = 1, col = 1)\n",
    "fig.update_yaxes(title_text = 'Day', row = 1, col = 2)\n",
    "fig.update_layout(title = {'text': 'Cell Overlap Between Days', 'xanchor': 'center', 'y': 0.95, 'x': 0.5})\n",
    "fig.update_layout(coloraxis_colorbar = {'title': 'Percent Overlap'})\n",
    "## Set boundaries for lines\n",
    "boundaries = [5, 10, 15]\n",
    "for boundary in boundaries:\n",
    "        fig.add_vline(x=boundary+0.5, line_width=2, line_color='red', opacity=1)\n",
    "        fig.add_hline(y=boundary+0.5, line_width=2, line_color='red', opacity=1) \n",
    "fig.update_xaxes(dtick=1)\n",
    "fig.update_yaxes(dtick=1)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load cell activity for each session\n",
    "## Set path and mouse\n",
    "path = '/media/caishuman/csstorage3/Austin/CircleTrack/MultiCon_AfterHours/MultiCon_EEG1/{}_data'.format(session_type)\n",
    "neural_dictionary = {}\n",
    "for mouse in tqdm(mouse_list):\n",
    "    neural_dictionary[mouse] = ctn.import_mouse_neural_data(path, mouse, key_file = key_file, session = '20min', plot_frame_usage = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Calculate spearman correlations between the average activity in each session\n",
    "param_distance = 5\n",
    "activity_dictionary = {}\n",
    "for mouse in tqdm(mouse_list):\n",
    "    activity_summary = []\n",
    "    neural_data = neural_dictionary[mouse]\n",
    "    mappings = pd.read_pickle('../../cross_registration_results/{}_data/{}/mappings_{}.pkl'.format(session_type, mouse, param_distance))\n",
    "    for d1,d2 in itertools.combinations_with_replacement(neural_data, r = 2):\n",
    "        session_one = neural_data[d1]\n",
    "        session_two = neural_data[d2]\n",
    "        if d1 == d2:\n",
    "            cell_ids = mappings.session[[neural_data[d1].session.values.tolist(), neural_data[d2].session.values.tolist()]].dropna(how = 'any').drop_duplicates().reset_index(drop = True)\n",
    "        else:\n",
    "            cell_ids = mappings.session[[neural_data[d1].session.values.tolist(), neural_data[d2].session.values.tolist()]].dropna(how = 'any').reset_index(drop = True)\n",
    "        ## Select unit_ids based on cell_ids\n",
    "        first_session = session_one.sel(unit_id = np.array(cell_ids.iloc[:, 0]))\n",
    "        second_session = session_two.sel(unit_id = np.array(cell_ids.iloc[:, 1]))\n",
    "        ## Calculate correlation activity\n",
    "        res = ctn.calculate_activity_correlation(first_session, second_session, test = 'spearman')\n",
    "        ## Add to list\n",
    "        tmp = [d1, d2, res[0], res[1]]\n",
    "        tmp2 = [d2, d1, res[0], res[1]]\n",
    "        activity_summary.append(tmp)\n",
    "        activity_summary.append(tmp2)\n",
    "    activity_summary = pd.DataFrame(activity_summary, columns = ['session_id1', 'session_id2', 'statistic', 'pvalue'])\n",
    "    activity_dictionary[mouse] = activity_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plot average population vector correlations between sessions\n",
    "fig = make_subplots(rows = 1, cols = 2, subplot_titles = (mouse_list[0], mouse_list[1]))\n",
    "for mouse in tqdm(mouse_list):\n",
    "    data = activity_dictionary[mouse]\n",
    "    matrix = data.pivot_table(index = 'session_id1', columns = 'session_id2', values = 'statistic')\n",
    "    matrix = matrix.sort_values(by = 'session_id1')\n",
    "    matrix = matrix.sort_values(by = 'session_id2', axis = 1)\n",
    "    if mouse == mouse_list[0]:\n",
    "        fig.add_trace(go.Heatmap(z = matrix.values, x = matrix.index, y = matrix.columns, coloraxis = 'coloraxis'), row = 1, col = 1)\n",
    "    elif mouse == mouse_list[1]:\n",
    "        fig.add_trace(go.Heatmap(z = matrix.values, x = matrix.index, y = matrix.columns, coloraxis = 'coloraxis'), row = 1, col = 2)\n",
    "fig.update_layout(template = 'simple_white', width = 1600, height = 800, coloraxis = {'colorscale': 'Viridis'})\n",
    "fig.update_xaxes(title_text = 'Day', row = 1, col = 1)\n",
    "fig.update_xaxes(title_text = 'Day', row = 1, col = 2)\n",
    "fig.update_yaxes(title_text = 'Day', row = 1, col = 1)\n",
    "fig.update_yaxes(title_text = 'Day', row = 1, col = 2)\n",
    "fig.update_layout(title = {'text': 'PVC Between Sessions', 'xanchor': 'center', 'y': 0.95, 'x': 0.5})\n",
    "fig.update_layout(coloraxis_colorbar = {'title': 'r value'})\n",
    "fig.show()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Play around with cell dynamics:\n",
    "## Interested in seeing the average number of spikes per cell distribution\n",
    "neural_data = neural_dictionary['mc_EEG1_01']['A2']\n",
    "neural_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plot individual cell activity across the session\n",
    "time_vec = np.arange(0, neural_data.shape[1])\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x = time_vec, y = neural_data.values[17]))\n",
    "fig.update_layout(template = 'simple_white')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## zscore data\n",
    "zdata = zscore(neural_data.values, axis = 1)\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x = time_vec, y = zdata[4]))\n",
    "fig.update_layout(template = 'simple_white')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zdata[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_thresh = 2\n",
    "test = zdata > z_thresh\n",
    "ans = zdata[0][test[0]]\n",
    "len(ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Loop through every cell and get the number of spikes across the session for each cell\n",
    "z_thresh = 2\n",
    "boolean = zdata > z_thresh ## binarize for spike vs no spike\n",
    "spikes = []\n",
    "for i in np.arange(0, zdata.shape[0]):\n",
    "    ans = zdata[i][test[i]]\n",
    "    spikes.append(len(ans))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "fig.add_trace(go.Histogram(x = spikes))\n",
    "fig.update_layout(template = 'simple_white', width = 500, height = 500)\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('calcium')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3dd79809ed5cd61d934147ab957e61d0ca120bfb2ec4541704fc20d5f29e686c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
